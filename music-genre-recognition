{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.4.0"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1032238,"sourceType":"datasetVersion","datasetId":568973}],"dockerImageVersionId":30749,"isInternetEnabled":true,"language":"r","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This R environment comes with many helpful analytics packages installed\n# It is defined by the kaggle/rstats Docker image: https://github.com/kaggle/docker-rstats\n# For example, here's a helpful package to load\n\nlibrary(tidyverse) # metapackage of all tidyverse packages\n\n# In this notebook we will use .... to indicate that you have to write the code yourself. \n# To make that work we execute the following code (that you can just ignore)\nmakeActiveBinding(\"....\", function() {warning(\"\\n\\n\\t\\t*** You have to write this code still. *** \\n\\n\", call.=FALSE); invisible(NULL)}, baseenv())\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nlist.files(path = \"../input\")\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature filtering — the Wrong and the Right Way\n\nIn this practical we're going to intoduce the concept of feature filtering — something that you may have already done naturally (e.g., in your bachelor), but comes with some subtlety  when it comes to cross-validation.\n\nWe are trying to build a model that can recognize different genres of music from the sound waves, and try to limit the number of features to use.\n\n## Importing and Inspecting the Data\n\nThe data consists of music files with fragments of music of different genres. The files are arranged in different folders. To get an impression of the files and the folder structure, let's list the names of all the .wav files:","metadata":{}},{"cell_type":"code","source":"# To inspect the music files, let's list all the .wav files\nwav_files = list.files(\"../input\",pattern=\".wav$\",recursive=T, full=T) \nwav_files |> head(3)\nwav_files |> tail(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We see the genre names in the file paths; extract them to list which genres are present\nstr_extract(wav_files, \"genres_original/([a-z]+)\",1) |> unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To get a more tangible impression of what we're working with, let's play an audio file. \n\nYou can change the genre to one of the ones listed in the output of the previous code cell to hear a different genre, to see if you can (easily) hear the difference.","metadata":{}},{"cell_type":"code","source":"## Play one of the sound files with an HTML audio player\n\n# Filter filenames for genre\ngenre_files = Filter(function(x) grepl(\"classical\", x), wav_files) \n                     \n# Copy one of the genre file to the working directory\nfile.copy(genre_files |> sample(1), \"audio.wav\", overwrite=TRUE)\n                     \n# Display a HTML audio player\nIRdisplay::display_html('<audio controls autoplay=\"true\"><source src=\"audio.wav\" type=\"audio/wav\">Your browser does not support the audio element.</audio>')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Extracting features from these files would involve the type of features that are typical for time signals: time domain features and frequency domain features — both the (relatively) simple ones we've seen before, as well as more advanced that have been found especially useful for music as they mimic the way the human ear extracts information from music (such as short time Fourier transform (\"STFT\") and `ceptrum`).\n\nThese are not the topic of this practical and such features have been precomputed for you. They are stored in a .csv file:\n","metadata":{}},{"cell_type":"code","source":"# There are two .csv files with pre-extracted features:\n# - one with features extracted from 3 sec of samples\n# - one with features extracted from 30 sec of samples\ncsv_files = list.files(\"../input\", pattern = \".csv$\", recursive=TRUE, full=TRUE)\ncsv_files","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> <span style=font-size:200%>👩‍💻🧑‍💻🖱️</span> **Task**: Import the the 30 sec feature file in a tibble (data frame). Make sure to print the head of the data frame.","metadata":{}},{"cell_type":"code","source":"# We'll work with the 30 sec based features\ndat = read_csv(csv_files[2])\ndat |> head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Apparently `label` is the variable in the data frame that is the target.\n\n> <span style=font-size:200%>🧑‍💻👩‍💻🖱️</span> **Task**: Inspect the numbers of examples and the number features in the data frame.","metadata":{}},{"cell_type":"code","source":"# How many examples and how many features are available?\n\n.... ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Solution\n# How many examples and how many features are available?\ndim(dat)","metadata":{"trusted":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> <span style=font-size:200%>👩‍💻🧑‍💻🖱️</span> **Task**: Inspect the data frame with a summary. Check the data for odd values","metadata":{}},{"cell_type":"code","source":"# Inspect the feature value characteristics\n\n....","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Solution\n# Inspect the feature value characteristics\noptions(width=110)\nsummary(dat)","metadata":{"trusted":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It's always a good idea to get an impression of the dependencies between features","metadata":{}},{"cell_type":"code","source":"# Look at the correlations to inspect the dependencies between features\noptions(repr.plot.width=16, repr.plot.height=16)\ndat |> select_if(is.numeric) |> cor() |> corrplot::corrplot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Look at the label distribution\ntable(dat$label) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"To preprocess the features we'll use the tools provided by the `caret` package.","metadata":{}},{"cell_type":"code","source":"library(caret)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we tend to always do, we'll look to remove near zero variance features and highly correlated features, and we will center and scale the features. We didn't seen any missing values in the data import (no `NA`s), so imputation or ommission is not necessary. We may consider feature reduction (PCA), but we can reconsider that later.","metadata":{}},{"cell_type":"code","source":"preproc = preProcess(dat, method=c(\"nzv\",\"corr\",\"center\",\"scale\"), cutoff=.95)\npreproc\ndat2 = predict(preproc, dat |> select(-filename))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature filtering\n\nThere are 60 features, and 1000 examples. For most classification models that we consider below the number of parameters that need to be estimated grows more than proportionally to the number of features. For instance, for Linear Discriminant Analysis with $K$ categories with $p$ features we need to estimate $K$ centroids of length $p$ and a $p\\times p$ covariance matrix — this amounts to $Kp + p(p+1)/2$ parameters which grows ***quadratically*** with $p$. \n\nThe number of parameters is one of the most important determinants of the flexibility of a model. With more parameters, the model can adapt more closely to the training data. But this also means it is more sensitive to random noise — increasing variance. Hence, the number of parameters is one of the most important determinants of the _variability of the fit_ of the model. In optimizing the bias-variance trade-off, we therefore may trade in some of the flexibility-induced variance for a bit more bias by reducing the number of features $p$. \n\nAs we have already seen, we can do this for example by means of PCA. An alternative method however, that sometimes works better is by filtering out features that are _statistically associated_ with the target variable — at least when considered on a univariate basis. \n\nThis is the procedure alluded to in the lecture videos for chapter 5 by Hastie and Tibshirani for which they warn that you can do it _right_ or _wrong_:\n","metadata":{}},{"cell_type":"code","source":"# Run this cell to see the video and slides\nIRdisplay::display_html('<table><tr><td><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/r64tRyHFAJ8?si=AR8K2NWRgDjt7xPE\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></td><td><iframe src=\"https://hastie.su.domains/ISLR2/Slides/Ch5_Resampling_Methods.pdf#page=25.00\" width=\"450\" height=\"315\"></iframe></td></tr></table>')","metadata":{"trusted":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We're going to see difference between doing it wrong and doing it right. The wrong way leads to a too optimistic error estimate _even when we use cross-validation_. The right way is to cross-validate the _entire model building_ pipeline, including this feature filtering step. (What else belongs to the model building pipeline?)\n\nHow do we determine the correlation with the class labels? A standard way to determine with a categorical variable and a numerical variable are statistically related is, of course, the one-way ANOVA that you know from your bachelor studies: If the categories have statistcally significant means of the variable, the two are statistically associated. \n\nWhat we are going to do is to test each feature in our data frame for a statistically significant relation with the target variable `label` using a one-way ANOVA. We'll retain the features that have the strongest statistical association (as measured by the p-values). First we'll do it for one feature to remind us how to run an ANOVA in R again, and then we'll ask an LLM to help us define a function that takes a numerical and categorical array of equal length and runs an ANOVA and returns the p-value of that statistical test\n\n\n> <span style=font-size:200%>👩‍💻🧑‍💻🖱️</span> **Task**: First write the code for running a simple ANOVA with `length` as dependent variable and `label` as group variable and check if the means of `length` differ significantly between the different categories.","metadata":{}},{"cell_type":"code","source":"# Run an ANOVA and check\n\n....","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Solution\n\n# Run an ANOVA and check\naov(length ~ label, data = dat2) |> summary()","metadata":{"trusted":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Is there significant effect of `label`, and hence, are `label` and `length` correlated?\n\nThe next step is to extract the p-value from the ANOVA. Because we want to do this for all numerical features in the data frame we'll ask chatGPT to write a function that extracts the p-value\n\n> <span style=font-size:200%>👩‍💻🧑‍💻🖱️</span> **Task**: Ask chatGPT or your favorite LLM the following\n> \n> > _In R, write a function that takes a numerical array 'x' and an array 'y' that should be interpreted as a categorical variable, and that returns the p-value from a oneway ANOVA._\n> \n> The output is neat and well annotated, but it puts a lot of focus on this function, and it is a discardable function that we probably use only for this project. It would be nice if it could be a one-liner that is built using pipes (`|>`). We therefore ask to refine the function:\n> \n> > _Make it a one-liner and use pipes |>_\n>\n> Name your function `oneway_anova_p`. Add a comment on top to explain what it does.\n>\n> Test your function with `length` and `label`","metadata":{}},{"cell_type":"code","source":"## Paste your LLM answer\n\n# ==> LLM START src=\n\n# ... your comment to explain what the function does.\noneway_anova_p = .... \n\n# ==> LLM END\n\n....","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Solution\n\n## The answer looks like this:\n\n# ==> LLM START src=https://aichat.uva.nl/chat/2630OqvY6eCgXS7wafxB8W1gAgIMHZTeGGYH\n\n# Helper function to extract the p-value for the statistical relation between numeric x and categorical y\noneway_anova_p <- function(x, y) summary(aov(x ~ as.factor(y))) |> _[[1]][[\"Pr(>F)\"]][1]\n\n# ==> LLM END\n\n# Test the function\noneway_anova_p(dat2$length, dat2$label)\n","metadata":{"trusted":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nNow we want to run the ANOVA on all the features and select the ones that are the most significant (i.e., that have the smallest p-values).\n\n> <span style=font-size:200%>👩‍💻🧑‍💻🖱️</span> **Task**: Apply `oneway_anova_p` to each feature in the data frame `dat2` with `label` as the grouping variable, and store the  names of the 38 features with the lowest p-values in the array `selected_features`.\n> \n> _Hint_: Use `sort()` on the p-values","metadata":{}},{"cell_type":"code","source":"# Select all features that have a statistically significant relation with the target\n\n.... #  Your code to compute the p-values\n\nselected_features = .... #  Your code to select the 38 most significant feature names\nprint(selected_features)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Solution\n\n# Select all features that have a statistically significant relation with the target\npvalues = dat2 |> select(-label) |> sapply(oneway_anova_p, y=dat2$label)\nselected_features <- pvalues |> sort() |> head(40) |> names()\nselected_features ","metadata":{"trusted":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cross-validating an LDA model — the _wrong_ way\n\nNow that we have the statistically significant features, we can fit a model using only these features. \n\nWe will use 5-fold cross-validation using `caret::train()`. To consistently use the same 5-folds across models for comparisson, we first create the folds using the `caret::createFolds()` function and pass the result to our `trainControl()` object:","metadata":{}},{"cell_type":"code","source":"set.seed(92025)\nidx = createFolds(dat$label, 5, returnTrain = TRUE)\ntrcntr = trainControl(\"cv\", 5, index=idx)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"`createFolds` requires the target variable `label` because it makes sure the folds each have examples from each of the categories.\n\nNext we cross-validate an LDA model:\n\n> <span style=font-size:200%>👩‍💻🧑‍💻🖱️</span> **Task**: Fit an LDA model with `caret::train()` all the featuress in `dat2`. Don't forget to use `trcntr` as the argument to `trControl`.","metadata":{}},{"cell_type":"code","source":"# Fit an LDA model using all the features\n\nfit_lda = ....\nfit_lda","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Solution\n\n# Fit an LDA model using all the features\nfit_lda = train(label ~ ., data=dat2, method=\"lda\", trControl=trcntr)\nfit_lda","metadata":{"trusted":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we'll fit an LDA again, but using only the top 38 features in `selected_features`:","metadata":{}},{"cell_type":"code","source":"# Fit an LDA with 5-fold cross-validation\ncolumns_to_keep = c(selected_features,\"label\") # columns in dat2 that we need\n\nfit_lda_reduced = train(label ~ ., data=dat2 |> select(all_of(columns_to_keep)), method=\"lda\", trControl=trcntr)\nfit_lda_reduced","metadata":{"trusted":true,"_kg_hide-input":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A slightly worse performance, but with only two-thirds of the features. Let's see if the performance difference is significant:","metadata":{}},{"cell_type":"code","source":"resamples(list(full=fit_lda, reduced=fit_lda_reduced)) |> diff() |> summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This suggests that our reduced model might be more robust (because less flexible) and not underperforming.\n","metadata":{}},{"cell_type":"markdown","source":"## Cross-validating an LDA model — the _right_ way\n\nNo we're going to do it the right way.  We're going to implement k-fold cross-validation by hand. Remember that each fold in `idx` contains the rows of the test set for that fold, and that the rows not in this set are the training set used for fitting the model. \n\n<!-- \n\nImplementing k-fold cross-validation means that \n\n1. for each fold in `idx` we fit the model on the training data,\n2. then we use that model to make predictions for the test data\n3. we calculate the prediction error and store the accuracy\n\n-->\n\nOnce we've implemented this cross-validation loop, we can very simply add feature filtering as an additional step.\n\nWe'll start with implementing k-fold cross-validation:\n\n> <span style=font-size:200%>🧑‍💻👩‍💻🖱️</span> **Task**: Your goal is to implement k-fold cross-validation using the folds already in `idx`. \n>\n> Ask your favorite LLM the following to help you break down the problem into steps that you have to\n> implement without providing any code.\n> > In R, I have a data frame `dat2`and have already created a folds object `idx` with `caret::createFolds(returnTrain=TRUE)`. Please give me a minimal, numbered list of steps to implement k-fold cross-validation from scratch. No code, explanations or extra text — just the steps.\n\n<details> <summary style=\"font-weight:800\">Solution</summary>\n\nChatGPT gives an answer somewhat like this:\n\n`==> LLM START src=https://chatgpt.com/share/68cbafa4-282c-8005-b4e2-643d55e64fd8`\n\n> 1. Split the data into training and validation sets using the fold indices.\n> 2. Fit the model on the training set.\n> 3. Generate predictions on the validation set.\n> 4. Compute the chosen performance metric for the validation set.\n> 5. Repeat steps 1–4 for each fold.\n> 6. Aggregate the performance metrics across folds.\n\n`==> LLM END`\n\nUvA chat gives\n\n`==> LLM START src=https://aichat.uva.nl/chat/0TwJd50JMAMOVe3uj9dLjEDJHAa6tYvYboUV`\n\n> 1. Split `dat2` into `k` folds using `idx`.\n> 2. Initialize an empty list to store results.\n> 3. For each fold:\n>    4. Use the fold as the test set.\n>    5. Use the remaining folds as the training set.\n>    6. Train the model on the training set.\n>    7. Evaluate the model on the test set.\n>    8. Store the evaluation results.\n> 9. Aggregate the results from all folds.\n\n`==> LLM END`\n\nAs you can see, the UvA LLM suggests to first split the data frame before the loop, whereas ChatGPT suggests to do this  inside the loop. We'll adopt the latter approach. Note that UvA chat tells us that we should initialize a list to store the resuls (the results from each iteration in the loop, that is). ChatGPT fails to tell us that, but we really should!\n\n</details>","metadata":{}},{"cell_type":"markdown","source":"\n\n> <span style=font-size:200%>👩‍💻🧑‍💻🖱️</span> **Task**: Implement the loop suggested by the LLM. Try to do it yourself. Use `train()` to fit the LDA model, but specify `trControl=trainControl(\"none\")` to turn of cross-validation inside `train()`.\n> If you get stuck and really don't know how to get further you can ask your LLM in a follow up\n> > Please, implement this directly in simple R code. Use `train()` to fit the LDA model, but specify `trControl=trainControl(\"none\")` to turn of cross-validation inside `train()`. No extraneous stuff, just the direct loop.\n> \n> _Note: If you don't limit an LLM such as with this last sentence LLMs tend to produce all kinds of extra stuff that not only takes up more time to understand, but is also hard to check and to debug (even for the LLM itself!)._","metadata":{}},{"cell_type":"code","source":"# Implement the CV loop suggested by your LLM\n\n\n.... # your code","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Solution\n\n# ==> LLM START src=https://chatgpt.com/share/68cbb656-7924-8005-9058-666f4a1570e0\nresults <- numeric(length(idx))\n\nfor (i in seq_along(idx)) {\n  train_idx <- idx[[i]]              # note I hadn't told ChatGPT that `idx` contains training indice, not test indices\n  train_data <- dat2[train_idx, ]\n  test_data  <- dat2[-train_idx, ]\n  \n  fit <- train(label ~ ., data = train_data,\n               method = \"lda\",\n               trControl = trainControl(method = \"none\"))\n  \n  preds <- predict(fit, newdata = test_data)\n  results[i] <- mean(preds == test_data$label)\n}\n\nresults\nmean(results)\n# === LLM END\n","metadata":{"trusted":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Note that the resulting mean accuracy is exactly as reported by `train()`\n\nNow we'll add in the feature filtering code:\n\n> <span style=font-size:200%>👩‍💻🧑‍💻🖱️</span> **Task**: Rewrite the loop from the previous code cell in the code cell below. _Try to do it from the top of your head, or at least type it over and do not copy-paste! This is the only way this gets into your muscle memory!_\n>\n> Then add the feature filtering steps that we developed above. Be sure to perform it on the training data for the current fold and not on the entire data set!","metadata":{}},{"cell_type":"code","source":"\nacc <- numeric(length(idx)) #\n\nfor (i in seq_along(idx)) {\n  tr <- idx[[i]]\n  \n  train_data <- dat2[ tr, ]\n  test_data  <- dat2[-tr, ]\n  \n  # apply feature selection code from earlier on training set \n  pvals <- train_data |> select(-label) |> sapply(oneway_anova_p, y=train_data$label)\n  selected <- pvals |> sort() |> head(40) |> names()\n  \n  # restrict to selected variables + outcome \n  train_sel <- train_data |> select(all_of(c(selected, \"label\")))\n  test_sel  <- test_data |> select(all_of(c(selected, \"label\")))\n  \n  # fit lda \n  fit <- train(label ~ ., data = train_sel, method = \"lda\", trControl=trainControl(\"none\"))\n  \n  # predict & store accuracy \n  acc[i] <- mean(predict(fit, test_sel) == test_sel$label)\n}\n\nmean(acc)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Clearly, the estimated accuracy is a (tiny) bit lower than suggested by _the wrong way_ above. That the difference is so small is due to the current data set in which the features were already highly selected to have very significant associations with the music genres.\n\n\nThe `caret` package has a function to do this loop for you, called `sbf()` (which stands for 'selection by filtering'), however, unfortunately this function can be a pain to work with and easily runs into errors.\n\nA core take home message from this practical is the following: If you are using k-fold cross validation to get a better estimate of the performance of the final model, you should include _all the steps that look at the data to make choices_. You are not cross-validating the model itself, but you are actually cross-validating _the model building procedure_. Once you've come to a final model, you run  the chosen model building procedure on the whole data set. The cross-validated performance (error rate, accuracy, MSE, etc.) is the (pessimistically biased) estimate of the final model performance.\n\n> <span style=font-size:200%>🧠🦾</span> **Think**: Why does _the wrong way_ give a too optimistic picture about the performance in general?\n>\n<div style=\"background:#00000005\"><br/>\n\n  ... _double tap to write your answer here_ ...\n\n<br/></div>","metadata":{}}]}